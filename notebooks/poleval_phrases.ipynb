{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from itertools import product\n",
    "from pathlib import Path\n",
    "\n",
    "import editdistance\n",
    "from tqdm import tqdm\n",
    "\n",
    "from textmining.index import DiskIndex, DiskPositionIndex\n",
    "from textmining.lemmatization import Lemmas\n",
    "from textmining.tokenization import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance\n",
    "import sys\n",
    "\n",
    "\n",
    "def scaled_editdist(ans, cor):\n",
    "    ans = ans.lower()\n",
    "    cor = cor.lower()\n",
    "    \n",
    "    return editdistance.eval(ans, cor) / len(cor)\n",
    "    \n",
    "def single_match(a, c):\n",
    "    if c.isdecimal():\n",
    "        return a == c\n",
    "    return scaled_editdist(a, c) < 0.5\n",
    "        \n",
    "def match(ans, cor):\n",
    "    return any(single_match(ans, c) for c in cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/poleval/pytania.txt\") as f:\n",
    "    questions = [l.lower().strip() for l in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/poleval/odpowiedzi.txt\") as f:\n",
    "    correct_answers = [x.lower().split('\\t') for x in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = Lemmas.from_file(Path(\"../data/lemmas.pickle\"))\n",
    "index = DiskIndex(lemmas.lemmatize, Path(\"../data/index\"))\n",
    "pos_index = DiskPositionIndex(lemmas.lemmatize, Path(\"../data/position_index\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, title_importance=1, exact_importance=1, total_importance=1):\n",
    "    docs = index.search(query)\n",
    "    qtokens = tokenize(query.lower())\n",
    "    qlemmas = {lemma for token in qtokens for lemma in index.lemmatize(token)}\n",
    "    for doc in docs:\n",
    "        doc.title_matching = 0\n",
    "        doc.exact_matching = 0\n",
    "        for token in tokenize(doc.title):\n",
    "            lemmas = set(index.lemmatize(token.lower()))\n",
    "            if qlemmas.intersection(lemmas):\n",
    "                doc.title_matching += 1\n",
    "            if token.lower() in qtokens:\n",
    "                doc.exact_matching += 1\n",
    "\n",
    "        doc.total_matching = doc.title_matching\n",
    "        for token in tokenize(doc.content):\n",
    "            lemmas = set(index.lemmatize(token.lower()))\n",
    "            if qlemmas.intersection(lemmas):\n",
    "                doc.total_matching += 1\n",
    "            if token.lower() in qtokens:\n",
    "                doc.exact_matching += 1\n",
    "    return sorted(\n",
    "        docs, reverse=True,\n",
    "        key=lambda d: (\n",
    "            title_importance * d.title_matching\n",
    "            + exact_importance * d.exact_matching\n",
    "            + total_importance * d.total_matching\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(question, title_importance=1, exact_importance=1, total_importance=1):\n",
    "    question_tokens = [token for token in tokenize(question.lower()) if len(token) > 1]\n",
    "    if question_tokens[0] == \"czy\":\n",
    "        return \"Tak\"\n",
    "    while question_tokens:\n",
    "        query = \" \".join(question_tokens)\n",
    "\n",
    "        for doc in search(query, title_importance, exact_importance, total_importance):\n",
    "            result = doc.title\n",
    "            res_tokens = tokenize(result.lower())\n",
    "\n",
    "            for t1, t2 in product(res_tokens, question_tokens):\n",
    "                if scaled_editdist(t1, t2) <= 0.5:\n",
    "                    break\n",
    "            else:\n",
    "                paren_index = result.find(\"(\")\n",
    "                if paren_index != -1:\n",
    "                    result = result[:paren_index]\n",
    "                return result\n",
    "        # if answer not found, remove first token of query\n",
    "        del question_tokens[0]\n",
    "    return \"nie mam pojęcia, sorry\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [06:16,  2.65it/s]\n"
     ]
    }
   ],
   "source": [
    "answers = list(tqdm(map(answer, questions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL SCORE: 93.0\n"
     ]
    }
   ],
   "source": [
    "N = len(correct_answers)\n",
    "score = 0.0\n",
    "\n",
    "for ans, cor in zip(answers, correct_answers):    \n",
    "    if match(ans, cor):\n",
    "        score += 1\n",
    "        \n",
    "print ('TOTAL SCORE:', score)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_answer(question, title_importance=1, exact_importance=1, total_importance=1):\n",
    "    question_tokens = [token for token in tokenize(question.lower()) if len(token) > 1]\n",
    "    if question_tokens[0] == \"czy\":\n",
    "        return \"Tak\"\n",
    "    while question_tokens:\n",
    "        query = \" \".join(question_tokens)\n",
    "\n",
    "        for doc in pos_index.search(query):\n",
    "            result = doc.title\n",
    "            res_tokens = tokenize(result.lower())\n",
    "\n",
    "            for t1, t2 in product(res_tokens, question_tokens):\n",
    "                if scaled_editdist(t1, t2) <= 0.5:\n",
    "                    break\n",
    "            else:\n",
    "                paren_index = result.find(\"(\")\n",
    "                if paren_index != -1:\n",
    "                    result = result[:paren_index]\n",
    "                return result\n",
    "        # if answer not found, remove first token of query\n",
    "        del question_tokens[0]\n",
    "    return \"nie mam pojęcia, sorry\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [15:03,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL SCORE: 93.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "phrase_answers = list(tqdm(map(phrase_answer, questions)))\n",
    "\n",
    "N = len(correct_answers)\n",
    "score = 0.0\n",
    "\n",
    "for ans, cor in zip(answers, correct_answers):    \n",
    "    if match(ans, cor):\n",
    "        score += 1\n",
    "        \n",
    "print ('TOTAL SCORE:', score)        "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e10d8761d460a58c74b37693efebb76e1438d903d92f15b2b3c03e822d0a63b9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
