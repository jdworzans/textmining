{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import unicodedata\n",
    "import multiprocessing as mp\n",
    "\n",
    "import nltk\n",
    "from spacy.lang.pl import Polish\n",
    "from spacy.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(\"../data/cytaty.txt.gz\", \"rt\") as f:\n",
    "    txt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = txt.replace(\"&lt;br\", \"\")\n",
    "txt = txt.replace(\"/&gt;\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(word):\n",
    "    return nltk.word_tokenize(word, \"polish\")\n",
    "    \n",
    "with mp.Pool(6) as p:\n",
    "    nltk_tokenized = p.map(word_tokenize, txt.split(\"\\n*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = Polish()\n",
    "tokenizer = Tokenizer(pl.vocab)\n",
    "\n",
    "with mp.Pool(6) as p:\n",
    "    spacy_tokenized = p.map(tokenizer, txt.split(\"\\n*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstrip(word: str):\n",
    "    if word and unicodedata.category(word[0]).startswith(\"P\"):\n",
    "        return lstrip(word[1:])\n",
    "    return word\n",
    "\n",
    "def rstrip(word: str):\n",
    "    if word and unicodedata.category(word[-1]).startswith(\"P\"):\n",
    "        return rstrip(word[:-1])\n",
    "    return word\n",
    "\n",
    "def strip(word: str):\n",
    "    return lstrip(rstrip(word))\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return list(filter(None, map(strip, sentence.split(\" \"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip(word: str):\n",
    "    idxs = [\n",
    "        idx\n",
    "        for c, idx in zip(map(unicodedata.category, word), range(len(word)))\n",
    "        if not c.startswith(\"P\")\n",
    "    ]\n",
    "    return word[min(idxs, default=0):max(idxs, default=len(word)) + 1]\n",
    "\n",
    "def tokenize(sentence: str):\n",
    "    return list(filter(None, map(strip, sentence.split(\" \"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp.Pool(6) as p:\n",
    "    tokenized = p.map(tokenize, txt.split(\"\\n*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tokens.txt\", \"wt\") as f:\n",
    "    for nltk_tokens, our_tokens, spacy_tokens in list(zip(nltk_tokenized, tokenized, spacy_tokenized)):\n",
    "        if nltk_tokens != our_tokens:\n",
    "            f.write(\"NLTK: \" + str(nltk_tokens) + \"\\n\")\n",
    "            f.write(\"NASZ: \" + str(our_tokens) + \"\\n\")\n",
    "            f.write(\"SPACY:\" + str(list(map(str, spacy_tokens))) + \"\\n\")\n",
    "            f.write(\"*\" * 30 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e10d8761d460a58c74b37693efebb76e1438d903d92f15b2b3c03e822d0a63b9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
